# Checklist: Metacat K8s 3.0.0 to 3.1.0 Upgrade Steps

> **= = = THIS IS A TEMPLATE - MAKE YOUR OWN COPY BEFORE CHECKING BOXES! = = =**

> ## IMPORTANT NOTES - before you begin...
> 1. **PURPOSE:** This ordered checklist is for upgrading an existing K8s Metacat v3.0.0 instance to
>    Metacat v3.1.0
>    * ***Very Important: Before starting a migration, you **must** have a fully-functioning
>      k8s installation of Metacat version 3.0.0/helm chart 1.1.x. Upgrade from other versions is
>      not supported. It is also assumed that your installation has already been fully indexed***
> 2. Some references below are specific to NCEAS infrastructure (e.g. CephFS storage); adjust as
>    needed for your own installation.
> 3. Assumptions: you have a working knowledge of Kubernetes deployment, including working with yaml
>    files, helm and kubectl commands, and your kubectl context is set for the target deployment
>    location

## 1. Values: Edit your existing values override file

**e.g. see the
[values-dev-cluster-example.yaml](../examples/values-dev-cluster-example.yaml)
file.**

- [ ] Remove any uid or gid overrides, since we're now adopting the defaults of 59996 for postgres
      and 59997 for metacat
- [ ] Temporarily disable probes until hashstore conversion is done


## 2. Prep Before Upgrading

- [ ] set `storage.hashstore.disableConversion: true`, so the hashstore converter won't run yet
- [ ] In the metacat database, verify that all the `systemmetadata.checksum_algorithm` entries are
  on the [list of supported
  algorithms](https://github.com/DataONEorg/hashstore-java/blob/main/src/main/java/org/dataone/hashstore/filehashstore/FileHashStore.java#L63)
  (NOTE: syntax matters! E.g. `sha-1` is OK, but `sha1` isn't):
    ```shell
    kubectl exec ${RELEASE_NAME}-postgresql-0 -- bash -c "psql -U metacat << EOF
      SELECT DISTINCT checksum_algorithm FROM systemmetadata WHERE checksum_algorithm NOT IN
            ('MD2','MD5','SHA-1','SHA-256','SHA-384','SHA-512','SHA-512/224','SHA-512/256');
    EOF"

    # then manually update each to the correct syntax; e.g:
    kubectl exec ${RELEASE_NAME}-postgresql-0 -- bash -c "psql -U metacat << EOF
      UPDATE systemmetadata SET checksum_algorithm='SHA-1' WHERE checksum_algorithm='SHA1';
    EOF"
    # ...etc
    ```


## 3. Upgrading - NOTE THERE WILL BE DOWNTIME!

### = = = = = = = = Downtime starts here = = = = = = = =

- [ ] Change ownership **ON CEPHFS** as follows:

    ```shell
    ## postgres (59996:59996) in postgresql data directory
    sudo chown -R 59996:59996 /mnt/ceph/repos/REPO-NAME/postgresql

    ## tomcat (59997:59997) in metacat directory
    sudo chown -R 59997:59997 data dataone documents logs
    ```

- [ ] ...then ensure all metacat `data` and `documents` files have `g+rw` permissions, otherwise,
  hashstore converter can't create hard links:

    ```shell
    sudo chmod -R g+rw data documents dataone
    ```

- [ ] `helm upgrade`, debug any startup and configuration issues
- [ ] Delete or comment out the `storage.hashstore.disableConversion:` setting, so the hashstore
    converter will run, and `helm upgrade` again. Allow hashstore upgrade to finish. (production
    machines took approx 0.16 seconds per object, but will likely be longer on dev cluster)

  > **NOTE:** while hashstore conversion is still in progress, it is expected for metacatUI to
  > display `Oops! It looks like there was a problem retrieving your search results.`, and for
  > `/metacat/d1/mn/v2/` api calls to display `Metacat has not been configured`

  > See [Tips, below](#monitor-hashstore-conversion-progress-and-completion) for
  > how to detect when hashstore conversion finishes

### = = = = = = = = Downtime ends here = = = = = = = =

- [ ] When hashstore conversion has finished, re-enable probes and helm upgrade to apply changes

---

## Tips:
### Monitor Hashstore Conversion Progress and Completion

* **To monitor progress:** check the number of rows in the `checksums` table: total # rows should
  be: `5 * (total objects)`, (approx; not accounting for conversion errors), where total object
  count can be found from `https://HOSTNAME/CONTEXT/d1/mn/v2/object`
   ```shell
   # get number of entries in `checksums` table -- should be approx 5*(total objects)
   kubectl exec ${RELEASE_NAME}-postgresql-0 -- bash -c "psql -U metacat << EOF
     select count(*) from checksums;
   EOF"
   ```
* **To detect when hashstore conversion finishes:**
  ```shell
  # EITHER CHECK STATUS FROM DATABASE...
  kubectl exec ${RELEASE_NAME}-postgresql-0 -- bash -c "psql -U metacat << EOF
    select storage_upgrade_status from version_history where status='1';
  EOF"

  # ...OR CHECK LOGS
  # If log4j root level is INFO
  egrep "\[INFO\]: The conversion took [0-9]+ minutes.*HashStoreUpgrader:upgrade"

  # If log4j root level is WARN, can also grep for this, if errors:
  egrep "\[WARN\]: The conversion is complete"
  ```

### Fix Hashstore Error - PID Doesn't Exist in `identifier` Table:

```shell
# If you see this in the metacat logs:
Pid <autogen pid> is missing system metadata. Since the pid starts with autogen and looks like to be
created by DataONE api, it should have the systemmetadata. Please look at the systemmetadata and
identifier table to figure out the real pid.
```
Steps to resolve:

1. Given the docid, get all revisions:
   ```sql
   select * from identifier where docid='<docid>';
   ```
2. Look for pid beginning 'autogen', and note its revision number
3. pid should be the `obsoleted_by` from the previous revision's system metadata:
   ```sql
   select obsoleted_by from systemmetadata where guid='<previous revision pid>';
   ```
4. Check by look at `obsoletes` from the following revision, if one exists:
   ```sql
   select obsoletes from systemmetadata where guid='<following revision pid>';
   ```
5. Check if systemmetadata table has an entry for autogen pid
   ```sql
   select checksum from systemmetadata where guid='<autogen pid>';
   ```
   ...and the checksum matches that of the original file, found in:
   ```shell
   /var/metacat/(data or documents)/<'autogen' docid>.<revision number>
   ```

#### = = = If these exist and do not match, STOP HERE AND INVESTIGATE FURTHER! = = =

6. If an autogen-pid entry was found, update it with the new pid:
   ```sql
   update systemmetadata set guid='<pid from steps 3 & 4>' where guid='<autogen pid>';
   ```
7. Replace the 'autogen' pid with the real pid in the 'identifier' table:
   ```sql
   update identifier set guid='<pid from steps 3 & 4>' where guid='<autogen pid>';
   ```
8. Set the hashstore conversion status back to `pending`:
   ```sql
   update version_history set storage_upgrade_status='pending' where status='1';
   ```
   ...and restart the metacat pod to re-run the hashstore conversion and generate the correct
   sysmeta file in hashstore

### Fix Hashstore Error - "null" error message:

This fix applies when the `/var/metacat/.metacat/generalError_{timestamp}.txt` file contains entries like this:

```
esa.34.1 null
```

...(where "null" is where the exception message should appear), and the logs contain this error for each one:

```
metacat 20250212-16:59:13: [ERROR]: Cannot move the object esa.34.1 to hashstore since null
    [edu.ucsb.nceas.metacat.admin.upgrade.HashStoreUpgrader:convert:541]
org.dataone.exceptions.MarshallingException: null
    [...]
Caused by: javax.xml.bind.MarshalException
    [...]
Caused by: org.xml.sax.SAXParseException: cvc-pattern-valid: Value '' is not facet-valid with
    respect to pattern '[\s]*[\S][\s\S]*' for type 'NonEmptyString'.
    [...]
```

Steps to resolve:

1. Ensure it was a pre-existing issue, by requesting the system metadata from the original source, e.g.: https://HOSTNAME/CONTEXT/d1/mn/v2/meta/GUID

2. Ensure nothing is missing from the `systemmetadata` and `xml_access` tables:
    ```sql
    select * from systemmetadata where guid='esa.34.1';
    select * from xml_access where guid='esa.34.1';
    ```

3. Check the `smreplicationpolicy` table:

    ```sql
    > \x
    Expanded display is on.
    > select * from smreplicationpolicy where guid='df35b.5.1';
    -[ RECORD 1 ]----------
    guid        | df35b.5.1
    member_node |
    policy      | blocked
    policy_id   | 16167
    -[ RECORD 2 ]----------
    guid        | df35b.5.1
    member_node |
    policy      | preferred
    policy_id   | 16168
    ```
    Note that the `member_node` is missing in each case. This is the cause of the error.

4. Try to determine what the intended value should be, from `metacat.properties`:

```properties
dataone.replicationpolicy.default.preferredNodeList=urn:node:KNB
dataone.replicationpolicy.default.blockedNodeList=
```

(in this case, we can assume that `preferred member_node` can be updated to `urn:node:KNB`, and that the `blocked` entry can be deleted)

```sql
> delete from smreplicationpolicy where guid='df35b.5.1' and policy='blocked';
DELETE 1
> update smreplicationpolicy set member_node='urn:node:KNB' 
                             where guid='df35b.5.1' AND policy='preferred';
>UPDATE 1
-- DON'T FORGET TO...
> COMMIT;
```

Finally, set the hashstore conversion status back to 'pending':

```sql
> update version_history set storage_upgrade_status='pending' where status='1';
UPDATE 1
> COMMIT;
```

...and restart the pod.
