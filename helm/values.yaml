## Default values for metacat.
## This is a YAML-formatted file.

## @section Metacat Application-Specific Properties
##
## The metacat section can contain any properties that metacat would expect to find in its
## configuration. These will be mounted in the container as metacat-site.properties,
## and will override the corresponding values in metacat.properties.
## The properties that have been pre-populated in this section comprise the minimum set of
## values needed to run the app and the test suite on a development machine.
##
## NOTE that certain credentials must also be provided, via Kubernetes Secrets, in order for
## metacat to function correctly. These credentials are listed in ./admin/secrets.yaml, in the
## form of environment variables expected by metacat at runtime. Also see the mappings in the
## `application.envSecretKeys` property in `metacat.properties`, to determine which metacat
## property corresponds to each of these environment variables
##
metacat:
  ## @param metacat.application.context The application context to use
  ## - for example, if your application is hosted at
  ## https://mydomain.org, and you define the context to be "metacat", then the url to access the
  ## application will be https://mydomain.org/metacat/
  ## NOTE: if changing this value, ensure the following paths are also updated to match:
  ##   readinessProbe.httpGet.path; livenessProbe.httpGet.path; ingress.hosts.paths
  ##
  application.context: metacat

  ## @param metacat.administrator.username The admin username that will be used to authenticate
  ## with the new metacat instance and apply any necessary setup steps, database upgrades etc.
  ## upon first run.
  ## NOTES:
  ## 1. The corresponding password must be set as a Secret (see ./admin/secrets.yaml), with the
  ##    key METACAT_ADMINISTRATOR_PASSWORD
  ## 2. This account will be created if it doesn't already exist in the `passwords.xml` file on
  ##    metacat's mounted PersistentVolume (see .Values.persistence)
  ## 3. This username MUST appear on the list of authorized administrators, otherwise
  ##    container startup will fail (see @param auth.administrators)
  ##
  administrator.username: admin@localhost

  ## @param metacat.auth.administrators A colon-separated list of admin usernames or LDAP-style DN
  ## (Distinguished Names) denoting the users who may log into metacat with administrator
  ## privileges.
  ##
  auth.administrators: admin@localhost:uid=jones,ou=Account,dc=ecoinformatics,dc=org

  ## @param metacat.database.connectionURI postgres DB URI (RELEASE PREFIX, or blank for sub-chart)
  ##
  ## Format:    jdbc:postgresql://<postgres-hostname>/<database-name>
  ##
  ## NOTES:
  ## 1. If you are NOT using the included postgresql sub-chart (i.e. `postgresql.enabled: false`),
  ##    then ensure `<database-name>` is set correctly, and use the following values for
  ##    `<postgres-hostname>`:
  ##    *  The fully qualified domain name, for an existing postgresql instance hosted outside the
  ##       current k8s cluster
  ##    *  `<servicename>.<namespace>.svc.cluster.local` for an existing postgresql instance within
  ##       the same k8s cluster (hence "RELEASE PREFIX" warning above; <servicename> likely is
  ##       prepended by the release name)
  ##    *  `host.docker.internal` for connecting to a dev instance running on localhost
  ##
  ## 2. If you ARE using the included postgresql sub-chart (i.e. `postgresql.enabled: true`),
  ##    leave this value unset (`database.connectionURI: ""`). It will then be automatically
  ##    populated as follows:
  ##    `<postgres-hostname>` will be set to to the current value of `.Release.Name`, prepended to
  ##                         `-postgresql-hl` (the name of the k8s headless service for the
  ##                         postgresql statefulset)
  ##    `<database-name>` will be set to the value of `postgresql.auth.database`
  ##    (Note: If you are using the postrgesql sub-chart but have changed the default name for the
  ##    headless service, then specify the full database.connectionURI as outlined in (1), above)
  ##
  database.connectionURI: ""

  ## @param metacat.guid.doi.enabled Allow users to publish Digital Object Identifiers at doi.org?
  ## If true, you will also need to define guid.doi.username $ guid.doi.password (see secrets.yaml)
  ## and either override or use the defaults in metacat.properties for all the entries that begin
  ## with: "guid.doi."
  ##
  guid.doi.enabled: true

  ## @param metacat.server.httpPort The http port exposed externally, if NOT using the ingress
  ## to allow connections from outside the k8s cluster (e.g. if you're using a loadBalancer
  ## service or kubectl proxy instead).
  ## If you ARE using the ingress (i.e. `.Values.ingress.enabled` is `true`), then leave this unset,
  ## and it will be auto-populated, depending whether or not you have TLS set up for the ingress
  ##
  server.httpPort: ""

  ## @param metacat.server.name The hostname for the server, as exposed by the ingress
  ## and seen by end users outside the cluster
  server.name: &external-hostname localhost

  ## @param metacat.solr.baseURL The url to access solr
  ## host.docker.internal is equivalent to "localhost"
  ##
  solr.baseURL: http://host.docker.internal:8983/solr

  ## @param metacat.replication.logdir Location for the replication logs
  ##
  replication.logdir: /var/metacat/logs

  ## @param metacat.dataone.certificate.fromHttpHeader.enabled Enable mutual auth with client certs
  ## For DataONE Replication -- mutual authentication with x509 client-side certs
  ## Also see `ingress.d1CaCertSecretName`
  ##
  dataone.certificate.fromHttpHeader.enabled: false

## @section Metacat Image, Container & Pod Parameters
##

## @param image.repository Metacat image repository
## @param image.tag Metacat image tag (immutable tags are recommended)
## @param image.pullPolicy Metacat image pull policy
image:
  #TODO pull from github container repo, e.g. see indexer: ghcr.io/dataoneorg/dataone-index-worker
  repository: metacat
  pullPolicy: IfNotPresent

  ## @param image.tag Overrides the image tag. Will default to the chart appVersion if set to ""
  ##
  tag: "DEVELOP"

  ## @param image.debug Specify if container debugging should be enabled (sets log level to "DEBUG")
  ## Set to true if you would like to see extra information in metacat/tomcat logs.
  ## * * WARNING - FOR TESTING ONLY! * * May result in secrets being printed to logs in plain text.
  ##
  debug: false

## @param imagePullSecrets [array] Optional list of references to secrets in the same namespace
## to use for pulling any of the images used by this chart
imagePullSecrets: []

## @param container.ports Optional list of additional container ports to expose within the cluster
## This section allows ports to be opened for development, debugging and testing purposes, in
## addition to the default metacat-web port 8080.
## Examples:
##     - containerPort: 5005
##       name: tc-remote-debug
##     - containerPort: 5701
##       name: hazelcast-port1
##     - containerPort: 5702
##       name: hazelcast-port2
##
## NOTE that port 8080 should not be included here - it is already defined in statefulset.yaml as:
##     - containerPort: 8080
##       name: metacat-web
##
container:
  ports: []

## ServiceAccount
## @param serviceAccount.create Should a service account be created to run Metacat?
## @param serviceAccount.annotations Annotations to add to the service account
## @param serviceAccount.name The name to use for the service account.
##                  If not set and create is true, a name is generated using the fullname template
##
serviceAccount:
  create: false
  annotations: {}
  name: ""

## @param podAnnotations Map of annotations to add to the pods
##
podAnnotations: {}

## PodSecurityContext
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
## @param podSecurityContext.enabled Enable security context
## @param podSecurityContext.fsGroup Group ID for the pod
##
podSecurityContext:
  enabled: false
  fsGroup:

## @param securityContext Holds pod-level security attributes and common container settings
securityContext: {}    #TODO
##  fsGroup: metacat
##  runAsUser: metacat
##  capabilities:
##    readOnlyRootFilesystem: true
##    runAsNonRoot: true
##   drop:
##   - ALL

## @param resources Resource limits for the deployment
##  We usually recommend not to specify default resources and to leave this as a conscious
## choice for the user. This also increases chances charts run on environments with limited
## resources, such as Minikube.
resources: {}

## @param tolerations Tolerations for pod assigment
tolerations: []

## @section Metacat Persistence
##
persistence:
  ## @param persistence.enabled Enable metacat data persistence using Persistent Volume Claims
  ## Always set to 'enabled: true' for production deployments.
  ##
  ## For development/testing ONLY: Setting 'enabled: false' will result in the use of a temporary
  ## 'emptyDir' for saving metacat's data. This means the data saved by metacat WILL BE LOST when
  ## the pod is deleted!
  ##
  enabled: true

  ## @param persistence.storageClass Storage class of backing PV
  ##
  ## If <storageClass> is defined,  storageClassName: <storageClass>
  ##
  ## If <storageClass> set to "-",  storageClassName: ""  -- which disables dynamic PV provisioning
  ##   (meaning claim can only be bound to an existing PV, not a dynamically-provisioned one) with
  ##   no class (no annotation, or one set equal to "")
  ##
  ## NOTE when using StatefulSet with a defaultClaimTemplate - leaving storageClass unset/null does
  ## NOT choose the default provisioner for dynamic provisioning of the underlying PV, as expected!
  ## Instead, inspect your cluster to see what storageClass is set as default:
  ##    $  kubectl get storageclass
  ## ...and then explicitly set storageClass to match the name of the default storageclass
  ## (e.g. for Rancher Desktop, use:   storageclass: local-path)
  ##
  storageClass: local-path

  ## @param persistence.existingClaim Name of an existing Persistent Volume Claim to re-use
  ## Set a value for 'existingClaim' only if you want to re-use a Persistent Volume Claim that has
  ## already been set up by a k8s admin ahead of time.
  ## Leaving it blank will cause a pvc to be created dynamically using volumeClaimTemplates.
  ##
  existingClaim: ""

  ## @param persistence.accessModes PVC Access Mode for metacat volume
  ## Example:
  ##    accessModes:
  ##    - ReadWriteOnce   # allow only one node to mount in read/write mode
  ##    - ReadOnlyMany    # allow many nodes to mount in read-only mode
  ## ReadWriteOnce  is always required by metacat. ReadOnlyMany is useful for giving other
  ## services (e.g. metadig) read-only access to metacat data.
  ## Note that the underlying PersistentVolume (or pv auto-provisioner) must be able to provide
  ## these modes, in order for the PVC to bind successfully. (For Rancher Desktop, this means
  ## setting only ReadWriteOnce, not ReadOnlyMany)
  ##
  accessModes:
    - ReadWriteOnce

  ## @param persistence.size PVC Storage Request for metacat volume
  ##
  size: 1Gi

## @section Networking & Monitoring
##

## Ingress is a collection of rules that allow inbound connections to reach the endpoints defined
## by a backend. An Ingress can be configured to give services externally-reachable urls, load
## balance traffic, terminate SSL, offer name based virtual hosting etc.
##
ingress:
  ## @param ingress.enabled Enable or disable the ingress
  ##
  enabled: true

  ## @param ingress.className ClassName of the ingress provider in your cluster
  ##  Inspect available classes in your cluster using:    $ kubectl get ingressclasses
  ##
  ## className: "traefik" -- For Rancher Desktop (provided you have traefik enabled:
  ##     'preferences' -> 'kubernetes' -> 'enable traefik')
  ## className: "nginx" -- For production, or to use certificates locally. Also:
  ## - disable traefik ('preferences' -> 'kubernetes' -> uncheck 'enable traefik')
  ## - install nginx:
  ##   $  helm upgrade --install ingress-nginx ingress-nginx  \
  ##        --repo https://kubernetes.github.io/ingress-nginx \
  ##        --namespace ingress-nginx --create-namespace)
  ##
  className: "traefik"

  ## @param ingress.hosts [array] A collection of rules mapping different hosts to the backend.
  ##        host: The external hostname exposed by the ingress and seen by clients,
  ##              that is mapped to this ingress via DNS
  ##        paths: A collection of rules mapping different paths on this host
  ##              to the backend. For each host, provide mappings to match incoming request paths:
  ##              path:     the url path used to identify the mapping
  ##              pathType: determines the interpretation of the Path-matching:
  ##                        Exact: Matches the URL path exactly.
  ##                        Prefix: Requires URL to begin with the pattern, not including substrings
  ##                        (e.g. /foo/bar matches /foo/bar/baz, but does not match /foo/barbaz).
  ##
  ## NOTE: All paths will be mapped to port 8080 of the `metacat-hl` headless service
  ##
  hosts:
    - host: *external-hostname
      paths:
        - path: "/metacat"
          pathType: Prefix
        # remove the following if you do NOT want metacatui to be exposed & accessed via /metacatui
        # (default knb skin)
        - path: "/metacatui"
          pathType: Prefix

  ## @param ingress.annotations Annotations for the ingress
  ##
  ## NOTE: You do NOT need to do anything here for DataONE mutual authentication with x509
  ## client-side certs, provided:
  ## 1. you are using the Kubernetes open source community version of the nginx ingress
  ##    (see https://github.com/kubernetes/ingress-nginx )
  ## 2. you have set ingress.clasName to nginx, and
  ## 3. you have set up ingress.tls correctly for https access, and
  ## 4. you have set the correct parameters in the metacat section
  ##    (see `metacat.dataone.certificate.fromHttpHeader.enabled`, above, for more details)
  ##
  annotations: {}

  ## @param ingress.tls The TLS configuration
  #### example ###
  ##  tls:
  ##    - hosts:
  ##        - *external-hostname
  ##      secretName: tls-secret
  tls: []

  ## @param ingress.d1CaCertSecretName Name of Secret containing DataONE CA certificate
  ## For DataONE Replication -- mutual authentication with x509 client-side certs.
  ## Also see `metacat.dataone.certificate.fromHttpHeader.enabled`
  d1CaCertSecretName: ca-secret

## Optional service in addition to the Headless ClusterIP that is required for StatefulSet
service:
  ## @param service.enabled Enable another optional service in addition to headless svc
  enabled: false

  ## @param service.type Kubernetes Service type. Defaults to ClusterIP if not set
  type: LoadBalancer

  ## @param service.clusterIP IP address of the service. Auto-generated if not set
  ## Valid values are "None", empty string (""), or a valid IP address. Setting this to "None"
  ## makes a "headless service" (no virtual IP). Using empty string ("") will auto-generate an IP
  ##
  clusterIP: ""

  ## @param service.ports [array] The port(s) to be exposed
  ##        service.ports.port The port to expose
  ##        service.ports.name A unique name to identify this port.
  ##        service.ports.targetPort the name (preferred) or number of the container
  ##                                 port where traffic will be sent.
  ##
  ports:
    - name: http-port
      port: 8080
      targetPort: metacat-web
    - name: hazelcast-test1
      port: 5701
      targetPort: hazelcast-port1

## LivenessProbe
## Periodic probe of container liveness. Container will be restarted if the probe fails.
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
##
## @param livenessProbe.enabled Enable livenessProbe for Metacat container
## @param livenessProbe.httpGet.path The url path to probe.
##                Note that the context (first path element) must match the
##                value of metacat.application.context
## @param livenessProbe.httpGet.port The named containerPort to probe
##                as defined in ./templates/statefulset.yaml
## Optional values:
##        livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
##        livenessProbe.periodSeconds Period seconds for livenessProbe
##        livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
##        livenessProbe.failureThreshold Failure threshold for livenessProbe
##        livenessProbe.successThreshold Success threshold for livenessProbe
##
##
livenessProbe:    ## TODO - refine and/or create new lightweight healthcheck route
  enabled: true
  httpGet:
    path: /metacat/
    port: metacat-web

## ReadinessProbe
## Periodic probe of container service readiness. If the probe fails, container will be removed
## from service endpoints (but will not be restarted unless livenessProbe fails)
##
## @param readinessProbe.enabled Enable readinessProbe for Metacat container
## @param readinessProbe.httpGet.path The url path to probe.
##                Note that the context (first path element) must match the
##                value of metacat.application.context
## @param readinessProbe.httpGet.port The named containerPort to probe
##                as defined in ./templates/statefulset.yaml
## Optional values:
##        readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
##        readinessProbe.periodSeconds Period seconds for readinessProbe
##        readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
##        readinessProbe.failureThreshold Failure threshold for readinessProbe
##        readinessProbe.successThreshold Success threshold for readinessProbe
##
readinessProbe:    ## TODO - refine and/or create new lightweight healthcheck route
  enabled: true
  httpGet:
    path: /metacat/admin
    port: metacat-web

## @section Postgresql Sub-Chart
postgresql:
  ## @param postgresql.enabled enable the postgresql sub-chart
  ## set to false if you want to connect to your own existing postgresql deployment (and ensure
  ## metacat.database.connectionURI is set accordingly).
  ## Once the postgres container is running, test using:
  ##   $  kubectl exec -it <postgresql-pod-name> -- psql  -U <username>  -d <databasename>
  ## (or from a shell in metacat container: psql  -U <username>  -h <pghostname> <databasename>)
  ##
  enabled: true

  auth:
    ## @param postgresql.auth.username Username for accessing the database used by metacat
    ## For the corresponding password, see POSTGRES_PASSWORD in secrets.yaml
    ## (These values are also used by metacat to authenticate)
    ## NOTE: config in postgresql.primary.pgHbaConfiguration must allow the username defined here!
    ##
    username: metacat

    ## @param postgresql.auth.database The name of the database used by metacat.
    ## Make sure metacat.database.connectionURI is using this value
    ##
    database: metacat

    ## @param postgresql.auth.existingSecret Secrets location for postgres password (RELEASE PREFIX)
    ## For example, if the release name is 'myrelease', secret name would be 'myrelease-secrets'
    ## Typically the existing secrets also used by metacat - see ./admin/secrets.yaml
    ##
    existingSecret: mc-secrets

    ## @param postgresql.auth.secretKeys.userPasswordKey Identifies metacat db's account password
    ## within metacat's existing secrets
    ##
    secretKeys:
      userPasswordKey: POSTGRES_PASSWORD

      ## @param postgresql.auth.secretKeys.adminPasswordKey Dummy value - not used (see notes):
      ## Bitnami expects to find it in our 'existingSecret' secrets location, and fails otherwise
      ##
      adminPasswordKey: POSTGRES_PASSWORD

  ## @param postgresql.primary.pgHbaConfiguration PostgreSQL Primary client authentication
  ## configuration; ref: https://www.postgresql.org/docs/current/static/auth-pg-hba-conf.html
  ##
  primary:
    ## override the default pf_hba.conf with our own, to allow password auth, since this is the
    ## only method metacat currently supports (as of June 2023)
    pgHbaConfiguration: |
      host        metacat       metacat       0.0.0.0/0       password
      host        metacat       metacat       ::0/0           password
      local       all           all                           trust

    ## Postgresql persistence
    ## For explanatory notes, see top-level `persistence` section for metacat, and the bitnami
    ## postgresql README at https://github.com/bitnami/charts/blob/main/bitnami/postgresql/README.md
    ## @param postgresql.primary.persistence.enabled Enable data persistence using PVC
    ## @param postgresql.primary.persistence.existingClaim Existing PVC to re-use
    ## @param postgresql.primary.persistence.storageClass Storage class of backing PV
    ## @param postgresql.primary.persistence.size PVC Storage Request for postgres volume
    ##
    persistence:
      enabled: true
      storageClass: ""
      existingClaim: ""
      size: 1Gi
